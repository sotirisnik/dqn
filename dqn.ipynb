{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a fraction of gpu, otherwise it will allocate almost all available gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "def get_session(gpu_fraction=0.3):\n",
    "\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "KTF.set_session(get_session())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Breakout Atari Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-01 19:28:39,711] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make( 'Breakout-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess observation\n",
    "<p>1) Resize image</p>\n",
    "<p>2) Crop center image - do not keep score </p>\n",
    "<p>3) Grayscale image - reduce the rgb space to grayscale, save space </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12bba50198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def preprocess_observation( observation ):\n",
    "    res = cv2.resize( observation, (84,110) )#resize to 110x84\n",
    "    crop = res[18:110-8,:,:]#crop image\n",
    "    grayscale = cv2.cvtColor( crop, cv2.COLOR_BGR2GRAY )#apply grayscale\n",
    "    return ( grayscale )\n",
    "    \n",
    "observation = env.reset()#init state\n",
    "    \n",
    "plt.imshow( preprocess_observation( observation ), cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#model consists of 3 convolutional layers and 2 fully connected layers\n",
    "\n",
    "#ex. 32 filters of kernel(3,3), stride=4, input shape must be in format row, col, channels\n",
    "model.add( Conv2D(32, (8,8), strides=(4,4), padding='same', input_shape=(84,84,4) ) )\n",
    "model.add( Activation( 'relu' ) )\n",
    "\n",
    "model.add(Conv2D(64, (4,4), strides=(2,2), padding='same' ) )\n",
    "model.add( Activation( 'relu' ) )\n",
    "\n",
    "model.add(Conv2D(64, (3,3), strides=(1,1), padding='same' ) )\n",
    "model.add( Activation( 'relu' ) )\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add( Dense( env.action_space.n, kernel_initializer='uniform', activation='linear' ) )\n",
    "\n",
    "learning_rate = 0.00025\n",
    "model.compile(loss='mse', optimizer=RMSprop(lr=learning_rate), metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 21, 21, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3965440   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 4,045,476\n",
      "Trainable params: 4,045,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Game over! episode 1 done with total reward := 2.0\n",
      "2Game over! episode 2 done with total reward := 1.0\n",
      "3Game over! episode 3 done with total reward := 0.0\n",
      "4Game over! episode 4 done with total reward := 0.0\n",
      "5Game over! episode 5 done with total reward := 1.0\n",
      "6Game over! episode 6 done with total reward := 0.0\n",
      "7Game over! episode 7 done with total reward := 1.0\n",
      "8Game over! episode 8 done with total reward := 1.0\n",
      "9Game over! episode 9 done with total reward := 1.0\n",
      "10Game over! episode 10 done with total reward := 2.0\n",
      "11Game over! episode 11 done with total reward := 1.0\n",
      "12Game over! episode 12 done with total reward := 2.0\n",
      "13Game over! episode 13 done with total reward := 3.0\n",
      "14Game over! episode 14 done with total reward := 0.0\n",
      "15Game over! episode 15 done with total reward := 3.0\n",
      "16Game over! episode 16 done with total reward := 4.0\n",
      "17Game over! "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "max_reward = 0.0\n",
    "\n",
    "start_episode = 1\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "\n",
    "exploration_steps = 1000000\n",
    "\n",
    "epsilon_discount = ( epsilon - epsilon_min ) / exploration_steps\n",
    "\n",
    "MAX_SIZE = 10000#capacity of deque\n",
    "MIN_MIN_SIZE = 1000#min size for replay\n",
    "D = deque( maxlen=MAX_SIZE )#[]\n",
    "\n",
    "def load_deque():\n",
    "    global D\n",
    "    pkl_file = open( 'mydeque.pkl', 'rb')\n",
    "    D = pickle.load( pkl_file )\n",
    "    pkl_file.close()\n",
    "\n",
    "def save_deque():\n",
    "    output = open( 'mydeque.pkl', 'wb' )\n",
    "    pickle.dump( D, output )\n",
    "    output.close()\n",
    "\n",
    "def load_dqn_model():\n",
    "    global model\n",
    "    from keras.models import model_from_json\n",
    "    # load json and create model\n",
    "    json_file = open('model_background.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(\"model_background.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    #model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='mse', optimizer=RMSprop(lr=learning_rate), metrics=['accuracy'] )\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "episodes = []\n",
    "rewards = []\n",
    "epsilons = []\n",
    "\n",
    "def save_train():\n",
    "\n",
    "    global episodes, rewards, epsilons\n",
    "\n",
    "    #save [episodes, rewards, epsilons ] to csv file\n",
    "    d = {'episode': episodes, 'reward': rewards, 'epsilon': epsilons}\n",
    "    df = pd.DataFrame(data=d, index=None)\n",
    "\n",
    "    if not os.path.isfile('filename.csv'):\n",
    "        df.to_csv('filename.csv',header ='column_names', index=None)\n",
    "    else: # else it exists so append without writing the header\n",
    "        df.to_csv('filename.csv',mode = 'a',header=False, index=None)\n",
    "\n",
    "    episodes = []\n",
    "    rewards = []\n",
    "    epsilons = []\n",
    "\n",
    "    #save model to disk\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model_background.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model_background.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    #save deque to disk\n",
    "    save_deque()\n",
    "\n",
    "def load_train():\n",
    "\n",
    "    global start_episode, epsilon\n",
    "\n",
    "    #get last episode and epsilon\n",
    "    if not os.path.isfile('filename.csv'):\n",
    "        start_episode, epsilon = 1, 1\n",
    "    else: # else it exists so append without writing the header\n",
    "        df = pd.read_csv( 'filename.csv')\n",
    "\n",
    "        if len(df) == 0:\n",
    "            start_episode, epsilon = 1, 1\n",
    "        else:\n",
    "            epsilon = list( df['epsilon'].tail(1) )[0]\n",
    "            start_episode = list( df['episode'].tail(1) )[0] + 1\n",
    "\n",
    "    if os.path.isfile('model_background.json'):\n",
    "        load_dqn_model()\n",
    "\n",
    "    if os.path.isfile('mydeque.pkl'):\n",
    "        load_deque()\n",
    "\n",
    "load_train()\n",
    "\n",
    "#print( start_episode, epsilon )\n",
    "#print( type( start_episode ) )\n",
    "#print( type( epsilon ) )\n",
    "#print( model.summary() )\n",
    "#print( D )\n",
    "\n",
    "total_observe = 12000#total_episodes\n",
    "MIN_SIZE = 32\n",
    "\n",
    "def replay( ):\n",
    "\n",
    "    if len( D ) < MIN_MIN_SIZE:\n",
    "        return\n",
    "\n",
    "    samples = random.sample( D, MIN_SIZE )\n",
    "\n",
    "    for sample in samples:\n",
    "\n",
    "        observation, reward, done, new_observation, action = sample\n",
    "\n",
    "        y = model.predict( observation.reshape(  ( 1, 84, 84, 4) ) )\n",
    "\n",
    "        Q_next = model.predict( new_observation.reshape(  ( 1, 84, 84, 4) ) )\n",
    "\n",
    "        if done:\n",
    "            y[0,action] = reward\n",
    "        else:\n",
    "            y[0,action] = reward + gamma * ( np.max( Q_next[0]  ) )\n",
    "\n",
    "        neural_network_observation = observation.reshape(  ( 1, 84, 84, 4) )\n",
    "        model.fit( neural_network_observation, y, epochs=1, verbose=0 )\n",
    "        #model.train_on_batch( neural_network_observation, y )\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for episode in range( start_episode, total_observe+1 ):#3600*5):\n",
    "\n",
    "    observation = env.reset()\n",
    "\n",
    "    observation = preprocess_observation( observation )\n",
    "\n",
    "    recent_frames = deque(maxlen=4)\n",
    "\n",
    "    for i in range( 4 ):\n",
    "        recent_frames.append( observation )\n",
    "\n",
    "    total_reward = 0\n",
    "    \n",
    "    #print( episode )\n",
    "\n",
    "    while True:\n",
    "\n",
    "        #env.render()\n",
    "\n",
    "        stack_observation = np.stack(recent_frames,axis=0)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            Q = model.predict( stack_observation.reshape(  ( 1, 84, 84, 4) ) )[0]\n",
    "            action = np.argmax( Q )\n",
    "\n",
    "        new_observation, reward, done, info = env.step( action )\n",
    "\n",
    "        new_observation = preprocess_observation( new_observation )#apply preprocess\n",
    "\n",
    "        next_recent_frames = recent_frames.copy()\n",
    "        next_recent_frames.append( new_observation )\n",
    "        next_new_observation = np.stack(next_recent_frames,axis=0)\n",
    "\n",
    "        D.append( ( stack_observation, reward, done, next_new_observation, action ) )\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(  str(episode) + \"Game over!\", end= ' ' ),\n",
    "            replay()\n",
    "            episodes.append( episode )\n",
    "            rewards.append( total_reward )\n",
    "            epsilons.append( epsilon )\n",
    "            break\n",
    "\n",
    "        observation = new_observation\n",
    "\n",
    "        recent_frames.append( observation )\n",
    "\n",
    "    print( \"episode \" + str(episode) + \" done with total reward := \" + str(total_reward) )\n",
    "\n",
    "    epsilon = max( epsilon_min, epsilon - epsilon_discount )\n",
    "\n",
    "    if episode % 100 == 0 and episode > 1:\n",
    "        save_train()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"total time is \" + str( end - start ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:my_env]",
   "language": "python",
   "name": "conda-env-my_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
